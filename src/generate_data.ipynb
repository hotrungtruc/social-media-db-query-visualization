{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545434b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q sqlalchemy pymysql faker pandas numpy cryptography mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7cac1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import uuid\n",
    "import random\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1a32f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into MySQL successfully!\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import uuid\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Configuration: Define the number of records to generate\n",
    "NUM_USERS = 3000\n",
    "NUM_POSTS = 3000  # Assuming 1 post per user\n",
    "NUM_COMMENTS = 15000  # 5 comments per post on average\n",
    "NUM_REACTIONS = 45000  # 15 reactions per post\n",
    "NUM_MEDIA = 6000  # 2 media per post on average\n",
    "NUM_LOGINS = 20000  # 6-7 logins per user on average\n",
    "NUM_HASHTAGS = 1100  # Total hashtags used\n",
    "\n",
    "# EMOTION: Predefined reactions available for posts/comments\n",
    "emotions = [\n",
    "    {\"emotion_id\": str(uuid.uuid4()), \"emotion_name\": \"like\"},\n",
    "    {\"emotion_id\": str(uuid.uuid4()), \"emotion_name\": \"love\"},\n",
    "    {\"emotion_id\": str(uuid.uuid4()), \"emotion_name\": \"haha\"},\n",
    "    {\"emotion_id\": str(uuid.uuid4()), \"emotion_name\": \"sad\"},\n",
    "    {\"emotion_id\": str(uuid.uuid4()), \"emotion_name\": \"angry\"},\n",
    "    {\"emotion_id\": str(uuid.uuid4()), \"emotion_name\": \"wow\"},\n",
    "]\n",
    "emotion_df = pd.DataFrame(emotions)\n",
    "\n",
    "# USER generation\n",
    "users = []\n",
    "\n",
    "for _ in range(NUM_USERS):\n",
    "    user_id = str(uuid.uuid4())\n",
    "    age = int(np.clip(np.random.normal(28, 10), 16, 70))\n",
    "    gender = random.choices([\"Male\", \"Female\", \"Other\"], weights=[48, 48, 4])[0]\n",
    "\n",
    "    if gender == \"Male\":\n",
    "        avatar_id = random.randint(0, 99)\n",
    "        profile_picture = random.choice([\n",
    "            f\"https://randomuser.me/api/portraits/men/{avatar_id}.jpg\",\n",
    "            f\"https://picsum.photos/200?random={random.randint(1, 3000)}\"\n",
    "        ])\n",
    "    elif gender == \"Female\":\n",
    "        avatar_id = random.randint(0, 99)\n",
    "        profile_picture = random.choice([\n",
    "            f\"https://randomuser.me/api/portraits/women/{avatar_id}.jpg\",\n",
    "            f\"https://picsum.photos/200?random={random.randint(1, 3000)}\"\n",
    "        ])\n",
    "    else:\n",
    "        avatar_id = random.randint(0, 99)\n",
    "        profile_picture = random.choice([\n",
    "            f\"https://randomuser.me/api/portraits/men/{avatar_id}.jpg\",\n",
    "            f\"https://randomuser.me/api/portraits/women/{avatar_id}.jpg\",\n",
    "            f\"https://picsum.photos/200?random={random.randint(1, 3000)}\"\n",
    "        ])\n",
    "\n",
    "    created_time = fake.date_time_this_year()\n",
    "\n",
    "    users.append({\n",
    "        \"user_id\": user_id,\n",
    "        \"username\": fake.user_name(),\n",
    "        \"email\": fake.email(),\n",
    "        \"profile_picture\": profile_picture,\n",
    "        \"gender\": gender,\n",
    "        \"age\": age,\n",
    "        \"bio\": fake.text(max_nb_chars=100),\n",
    "        \"location\": fake.city(),\n",
    "        \"post_interest\": random.choice([\n",
    "            \"tech\", \"music\", \"entertainment\", \"sports\", \n",
    "            \"education\", \"travel\", \"food\", \"fashion\"\n",
    "        ]),\n",
    "        \"created_at\": created_time\n",
    "    })\n",
    "\n",
    "users_df = pd.DataFrame(users)\n",
    "\n",
    "\n",
    "# Define high engagement login periods (peak social media usage times)\n",
    "engagement_periods = [\n",
    "    {\"day\": \"Tuesday\", \"hours\": list(range(6, 12)) + list(range(15, 18))},\n",
    "    {\"day\": \"Wednesday\", \"hours\": list(range(6, 12)) + list(range(15, 18))},\n",
    "    {\"day\": \"Thursday\", \"hours\": list(range(6, 12)) + list(range(15, 18))}\n",
    "]\n",
    "\n",
    "# Generate random login times within high engagement periods\n",
    "def generate_login_time():\n",
    "    period = random.choice(engagement_periods)\n",
    "    hour = random.choice(period[\"hours\"])\n",
    "    minute = random.randint(0, 59)\n",
    "    second = random.randint(0, 59)\n",
    "    return fake.date_time_this_year().replace(hour=hour, minute=minute, second=second)\n",
    "\n",
    "# LOGIN: Generate fake login events\n",
    "logins = []\n",
    "for _ in range(NUM_LOGINS):\n",
    "    logins.append({\n",
    "        \"login_id\": str(uuid.uuid4()),\n",
    "        \"user_id\": random.choice(users_df['user_id']),\n",
    "        \"IP\": fake.ipv4_public(),\n",
    "        \"login_at\": generate_login_time()\n",
    "    })\n",
    "\n",
    "logins_df = pd.DataFrame(logins)\n",
    "\n",
    "# FOLLOW generation\n",
    "follows = []\n",
    "for user_id in users_df['user_id']:\n",
    "    expected_followers = int(np.clip(np.random.normal(100, 100), 5, 300))  # Followers with Gaussian distribution\n",
    "    followed_users = np.random.choice(\n",
    "        users_df[users_df['user_id'] != user_id]['user_id'],\n",
    "        size=min(expected_followers, len(users_df) - 1),\n",
    "        replace=False\n",
    "    )\n",
    "    for followee_id in followed_users:\n",
    "        follows.append({\n",
    "            \"follow_id\": str(uuid.uuid4()),\n",
    "            \"follower_id\": user_id,\n",
    "            \"followee_id\": followee_id,\n",
    "            \"created_at\": fake.date_time_this_year()\n",
    "        })\n",
    "\n",
    "follows_df = pd.DataFrame(follows).drop_duplicates(subset=[\"follower_id\", \"followee_id\"])\n",
    "\n",
    "# POST: Generate fake posts from users\n",
    "posts = []\n",
    "for _ in range(NUM_POSTS):\n",
    "    posts.append({\n",
    "        \"post_id\": str(uuid.uuid4()),\n",
    "        \"user_id\": random.choice(users_df['user_id']),\n",
    "        \"caption\": fake.sentence(),\n",
    "        \"location\": fake.city(),\n",
    "        \"post_category\": random.choice([\"tech\", \"music\", \"entertainment\", \"sports\", \"education\", \"travel\", \"food\", \"fashion\"]),\n",
    "        \"created_at\": fake.date_time_this_year()\n",
    "    })\n",
    "\n",
    "posts_df = pd.DataFrame(posts)\n",
    "\n",
    "# COMMENT: Generate comments on posts\n",
    "comments = []\n",
    "for _, post_row in posts_df.iterrows():\n",
    "    post_id = post_row['post_id']\n",
    "    post_created_at = post_row['created_at']\n",
    "    \n",
    "    expected_comments = int(np.clip(np.random.normal(loc=20, scale=20), 0, 200))  # Comments with Gaussian distribution\n",
    "    for _ in range(expected_comments):\n",
    "        comments.append({\n",
    "            \"comment_id\": str(uuid.uuid4()),\n",
    "            \"post_id\": post_id,\n",
    "            \"user_id\": random.choice(users_df['user_id']),\n",
    "            \"content\": fake.text(max_nb_chars=100),\n",
    "            \"created_at\": fake.date_time_between(start_date=post_created_at)\n",
    "        })\n",
    "\n",
    "comments_df = pd.DataFrame(comments)\n",
    "\n",
    "# REACTIONS: Generate post and comment reactions using weighted distributions\n",
    "post_reactions = []\n",
    "for _ in range(NUM_REACTIONS):\n",
    "    post_reactions.append({\n",
    "        \"post_react_id\": str(uuid.uuid4()),\n",
    "        \"user_id\": random.choice(users_df['user_id']),\n",
    "        \"post_id\": random.choice(posts_df['post_id']),\n",
    "        \"emotion_id\": random.choices(emotions, weights=[60, 20, 10, 5, 3, 2])[0]['emotion_id'],\n",
    "        \"created_at\": fake.date_time_this_year()\n",
    "    })\n",
    "\n",
    "post_reactions_df = pd.DataFrame(post_reactions).drop_duplicates(subset=[\"user_id\", \"post_id\"])\n",
    "\n",
    "comment_reactions = []\n",
    "for _ in range(NUM_REACTIONS):\n",
    "    comment_reactions.append({\n",
    "        \"comment_react_id\": str(uuid.uuid4()),\n",
    "        \"user_id\": random.choice(users_df['user_id']),\n",
    "        \"comment_id\": random.choice(comments_df['comment_id']),\n",
    "        \"emotion_id\": random.choices(emotions, weights=[50, 15, 20, 10, 3, 2])[0]['emotion_id'],\n",
    "        \"created_at\": fake.date_time_this_year()\n",
    "    })\n",
    "\n",
    "comment_reactions_df = pd.DataFrame(comment_reactions).drop_duplicates(subset=[\"user_id\", \"comment_id\"])\n",
    "\n",
    "# MEDIA: Generate random media attachments for posts\n",
    "media = []\n",
    "media_urls = set()  # To track media URLs and avoid duplicates\n",
    "for post_id in posts_df['post_id']:\n",
    "    expected_media = int(np.clip(np.random.normal(loc=3, scale=3), 1, 10))  # Media with Gaussian distribution\n",
    "    for _ in range(expected_media):\n",
    "        # Ensure unique media_url\n",
    "        while True:\n",
    "            media_url = f\"https://picsum.photos/200?random={random.randint(100, 10000)}\"\n",
    "            if media_url not in media_urls:\n",
    "                media_urls.add(media_url)\n",
    "                break\n",
    "        media.append({\n",
    "            \"media_id\": str(uuid.uuid4()),\n",
    "            \"post_id\": post_id,\n",
    "            \"media_url\": media_url,\n",
    "            \"media_type\": random.choice([\"image\", \"video\"]),\n",
    "            \"media_size_kb\": random.randint(100, 20480),\n",
    "            \"created_at\": fake.date_time_this_year(),\n",
    "        })\n",
    "\n",
    "media_df = pd.DataFrame(media)\n",
    "\n",
    "\n",
    "# HASHTAGS: Generate hashtags with Gaussian distribution (popular hashtags appear more frequently)\n",
    "popular_hashtags = [fake.word() for _ in range(10)]\n",
    "num_occurrences = np.abs(np.random.normal(loc=50, scale=15, size=len(popular_hashtags)).astype(int))\n",
    "\n",
    "hashtags = []\n",
    "for hashtag, count in zip(popular_hashtags, num_occurrences):\n",
    "    for _ in range(count):\n",
    "        hashtags.append({\n",
    "            \"hashtag_id\": str(uuid.uuid4()),\n",
    "            \"post_id\": random.choice(posts_df['post_id']),\n",
    "            \"hashtag_name\": hashtag.lower(),\n",
    "            \"created_at\": fake.date_time_this_year()\n",
    "        })\n",
    "\n",
    "hashtags_df = pd.DataFrame(hashtags).drop_duplicates()\n",
    "\n",
    "# Create SQLAlchemy engine (update the connection string with your correct credentials)\n",
    "engine = create_engine('mysql+pymysql://root:truc123@localhost:3306/social_network')\n",
    "\n",
    "# Insert data into MySQL with error handling\n",
    "try:\n",
    "    # Insert emotions first\n",
    "    emotion_df.to_sql('emotion', con=engine, if_exists='append', index=False, chunksize=500)\n",
    "\n",
    "    # Insert other data\n",
    "    users_df.to_sql('user', con=engine, if_exists='append', index=False, chunksize=500)\n",
    "    logins_df.to_sql('login', con=engine, if_exists='append', index=False, chunksize=500)\n",
    "    follows_df.to_sql('follow', con=engine, if_exists='append', index=False, chunksize=500)\n",
    "    posts_df.to_sql('post', con=engine, if_exists='append', index=False, chunksize=500)\n",
    "    post_reactions_df.to_sql('post_reaction', con=engine, if_exists='append', index=False, chunksize=500)\n",
    "    comments_df.to_sql('comment', con=engine, if_exists='append', index=False, chunksize=500)\n",
    "    comment_reactions_df.to_sql('comment_reaction', con=engine, if_exists='append', index=False, chunksize=500)\n",
    "    media_df.to_sql('media', con=engine, if_exists='append', index=False, chunksize=500)\n",
    "    hashtags_df.to_sql('hashtag', con=engine, if_exists='append', index=False, chunksize=500)\n",
    "\n",
    "    print(\"Data inserted into MySQL successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
